{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 240626\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from itertools import product\n",
    "import json\n",
    "from util_trans import *\n",
    "\n",
    "client = OpenAI(api_key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelResponder:\n",
    "    def __init__(self, model_path, ques_path_list, context_list, prompt_func_list=None, path=None):\n",
    "        self.batch_size = 100\n",
    "        self.model_path = model_path\n",
    "        if path is not None:\n",
    "            self.path = path\n",
    "        else:\n",
    "            self.path = os.path.basename(model_path)\n",
    "\n",
    "        if not prompt_func_list:\n",
    "            default_prompt_func = lambda inst,ques: f\"{ques}\"\n",
    "            prompt_func_list = [default_prompt_func]\n",
    "        self.prompt_func_list = prompt_func_list\n",
    "\n",
    "        indexed_ques_paths = list(enumerate(ques_path_list))\n",
    "        indexed_prompt_funcs = list(enumerate(self.prompt_func_list))\n",
    "        indexed_contexts = list(enumerate(context_list))\n",
    "        self.combinations = list(product(indexed_ques_paths, indexed_prompt_funcs, indexed_contexts))\n",
    "\n",
    "    def process_and_update(self, ques_dict, inst, prompt_func, batch_pbar):\n",
    "        max_retries = 5\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=self.model_path,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": inst},\n",
    "                        {\"role\": \"user\", \"content\": prompt_func(inst, ques_dict)},\n",
    "                    ],\n",
    "                    temperature=0.0000000000000000000000000000000000000000001,\n",
    "                    top_p=0.0000000000000000000000000000000000000000001,\n",
    "                    seed=100,\n",
    "                    )\n",
    "                batch_pbar.update(1)\n",
    "                output = response.choices[0].message.content\n",
    "                result = output_adjust(output)\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(e)\n",
    "                print(f\"Retrying... ({retries}/{max_retries})\")\n",
    "                time.sleep(60)\n",
    "                \n",
    "    def process_files(self):\n",
    "        for (ques_idx, ques_path), (prompt_idx, prompt_func), (context_idx, inst) in self.combinations:\n",
    "            with open(ques_path) as f:\n",
    "                exam = json.load(f)\n",
    "            filename=f'output/{self.path} [f{prompt_idx+1}_p{context_idx+1}_q{ques_idx+1}].json'\n",
    "            dataset = ExamDataset(exam, inst, prompt_func)\n",
    "            dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "            with tqdm(total=len(dataloader),desc=filename, leave=True, position=0) as pbar:\n",
    "                for i, batch in enumerate(dataloader):\n",
    "                    with ThreadPoolExecutor() as executor:\n",
    "                        with tqdm(total=len(batch), desc=f\"Batch {i}\", leave=True, position=0) as batch_pbar:\n",
    "                            results = list(executor.map(lambda ques: self.process_and_update(ques, inst, prompt_func, batch_pbar), batch))\n",
    "                    if os.path.exists(filename):\n",
    "                        with open(filename, 'r') as file:\n",
    "                            resp = json.load(file)\n",
    "                    else:\n",
    "                        resp = []\n",
    "                        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "                    for result in results:\n",
    "                        resp.append(result)\n",
    "                    with open(filename, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(resp, f, indent=4, ensure_ascii=False)\n",
    "                    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 0:   0%|          | 0/100 [00:00<?, ?it/s]         | 0/21 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 0: 100%|██████████| 100/100 [00:14<00:00,  6.80it/s]\n",
      "Batch 1: 100%|██████████| 100/100 [00:14<00:00,  6.67it/s] 1/21 [00:14<04:54, 14.71s/it]\n",
      "Batch 2: 100%|██████████| 100/100 [00:20<00:00,  4.94it/s] 2/21 [00:29<04:42, 14.88s/it]\n",
      "Batch 3: 100%|██████████| 100/100 [00:19<00:00,  5.24it/s] 3/21 [00:49<05:12, 17.33s/it]\n",
      "Batch 4: 100%|██████████| 100/100 [00:16<00:00,  6.19it/s] 4/21 [01:09<05:06, 18.04s/it]\n",
      "Batch 5: 100%|██████████| 100/100 [00:18<00:00,  5.44it/s] 5/21 [01:25<04:37, 17.37s/it]\n",
      "Batch 6: 100%|██████████| 100/100 [00:21<00:00,  4.56it/s] 6/21 [01:43<04:25, 17.72s/it]\n",
      "Batch 7: 100%|██████████| 100/100 [00:17<00:00,  5.62it/s] 7/21 [02:05<04:27, 19.10s/it]\n",
      "Batch 8: 100%|██████████| 100/100 [00:19<00:00,  5.24it/s] 8/21 [02:23<04:03, 18.71s/it]\n",
      "Batch 9: 100%|██████████| 100/100 [00:21<00:00,  4.75it/s] 9/21 [02:42<03:45, 18.83s/it]\n",
      "Batch 10: 100%|██████████| 100/100 [00:15<00:00,  6.40it/s]10/21 [03:03<03:34, 19.53s/it]\n",
      "Batch 11: 100%|██████████| 100/100 [00:18<00:00,  5.38it/s]11/21 [03:19<03:03, 18.35s/it]\n",
      "Batch 12: 100%|██████████| 100/100 [00:20<00:00,  4.77it/s]12/21 [03:37<02:45, 18.43s/it]\n",
      "Batch 13: 100%|██████████| 100/100 [00:17<00:00,  5.56it/s]13/21 [03:58<02:33, 19.20s/it]\n",
      "Batch 14: 100%|██████████| 100/100 [00:16<00:00,  6.04it/s]14/21 [04:16<02:11, 18.84s/it]\n",
      "Batch 15: 100%|██████████| 100/100 [00:15<00:00,  6.33it/s]15/21 [04:33<01:49, 18.17s/it]\n",
      "Batch 16: 100%|██████████| 100/100 [00:24<00:00,  4.05it/s]16/21 [04:49<01:27, 17.47s/it]\n",
      "Batch 17: 100%|██████████| 100/100 [00:17<00:00,  5.71it/s]17/21 [05:14<01:18, 19.64s/it]\n",
      "Batch 18: 100%|██████████| 100/100 [00:16<00:00,  5.90it/s]18/21 [05:31<00:57, 19.02s/it]\n",
      "Batch 19: 100%|██████████| 100/100 [00:21<00:00,  4.63it/s]19/21 [05:48<00:36, 18.41s/it]\n",
      "Batch 20: 100%|██████████| 100/100 [00:21<00:00,  4.66it/s]20/21 [06:10<00:19, 19.38s/it]\n",
      "output/gpt-4o-2024-05-13 [f1_p1_q1].json: 100%|██████████| 21/21 [06:31<00:00, 18.66s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path=\"gpt-4o-2024-05-13\"\n",
    "run = ModelResponder(model_path, ques_path_list, inst_list)\n",
    "run.process_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
