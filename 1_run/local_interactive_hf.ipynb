{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 240630\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from itertools import product\n",
    "import gc\n",
    "from format import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5,6,7,8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelResponder:\n",
    "    def __init__(self, model_path, exam_list_local, prompt_func_list, inst_list, path=None, quant=False, llama3=False, qwen=False, yi=False):\n",
    "        self.batch_size = 350\n",
    "        self.quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        if yi:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16 if (llama3 or qwen or yi) else torch.float16,\n",
    "            quantization_config=self.quantization_config if quant else None,\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "        if path is not None:\n",
    "            self.path = path\n",
    "        else:\n",
    "            self.path = os.path.basename(model_path)\n",
    "\n",
    "        self.gen_args = {\n",
    "            \"use_cache\": True,\n",
    "            \"max_new_tokens\": float('inf'),\n",
    "            \"do_sample\": False,\n",
    "            \"temperature\": None,\n",
    "            \"top_p\": None,\n",
    "            \"top_k\": None,\n",
    "        }\n",
    "\n",
    "        if llama3:\n",
    "            self.terminators = [\n",
    "                self.tokenizer.eos_token_id,\n",
    "                self.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "                ]\n",
    "            self.gen_args[\"eos_token_id\"] = self.terminators\n",
    "            self.gen_args[\"pad_token_id\"] = self.tokenizer.eos_token_id\n",
    "        if yi:\n",
    "            self.gen_args[\"eos_token_id\"] = self.tokenizer.eos_token_id\n",
    "            \n",
    "        indexed_ques_paths = list(enumerate(exam_list_local))\n",
    "        indexed_prompt_funcs = list(enumerate(prompt_func_list))\n",
    "        indexed_insts = list(enumerate(inst_list))\n",
    "\n",
    "        self.combinations = list(product(indexed_ques_paths, indexed_prompt_funcs, indexed_insts))\n",
    "\n",
    "    def process_files(self):\n",
    "        for (ques_idx, ques_path), (prompt_idx, prompt_func), (inst_idx, inst) in self.combinations:\n",
    "            with open(ques_path, encoding='utf-8') as f:\n",
    "                exam = json.load(f)\n",
    "            filename=f'output/{self.path} [f{prompt_idx+1}_p{inst_idx+1}_q{ques_idx+1}].json'\n",
    "            dataset = ExamDataset(exam, inst, prompt_func)\n",
    "            dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "            with tqdm(total=len(dataloader),desc=filename, leave=True, position=1) as pbar:\n",
    "                for i, batch in enumerate(dataloader):\n",
    "                    results=[]\n",
    "                    with tqdm(total=len(batch), desc=f\"Batch {i}\", leave=True, position=0) as batch_pbar:\n",
    "                        for ques_dict in batch:\n",
    "                            max_retries = 5\n",
    "                            retries = 0\n",
    "                            ques=ques_dict['input']\n",
    "                            while retries < max_retries:\n",
    "                                try:\n",
    "                                    inputs = self.tokenizer(ques, return_tensors=\"pt\", add_special_tokens=False).to(self.model.device)\n",
    "                                    input_len = len(inputs.input_ids[0])\n",
    "                                    output = self.model.generate(inputs.input_ids, **self.gen_args)\n",
    "                                    output_text = self.tokenizer.decode(output[0][input_len:], skip_special_tokens=True)\n",
    "                                    ques_dict['response'] = output_text\n",
    "                                    results.append(ques_dict)\n",
    "                                    batch_pbar.update(1)\n",
    "                                    break\n",
    "                                except RuntimeError as e:\n",
    "                                    current = f\"{ques_dict['year']}-{ques_dict['session']}-{ques_dict['question_number']}\"\n",
    "                                    if \"CUDA out of memory\" in str(e):\n",
    "                                        print(f\"CUDA OOM on '{current}'. Trying again after clearing cache.\")\n",
    "                                        torch.cuda.empty_cache()\n",
    "                                        retries += 1\n",
    "                                        if retries == max_retries:\n",
    "                                            print(f\"Skipping {current} after repeated OOM errors.\")\n",
    "                                            ques_dict['response'] = None\n",
    "                                            results.append(ques_dict)\n",
    "                                            batch_pbar.update(1)\n",
    "                                    else:\n",
    "                                        print(f\"Error on '{current}': {e}\")\n",
    "                                        raise\n",
    "\n",
    "                    if os.path.exists(filename):\n",
    "                        with open(filename, 'r', encoding='utf-8') as file:\n",
    "                            resp = json.load(file)\n",
    "                    else:\n",
    "                        resp = []\n",
    "                        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "                    resp += results\n",
    "                    with open(filename, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(resp, f, indent=4, ensure_ascii=False)\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "    def delete(self):\n",
    "        del self.model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.50it/s]\n",
      "Batch 0: 100%|██████████| 50/50 [00:04<00:00, 10.16it/s]        | 0/42 [00:00<?, ?it/s]\n",
      "Batch 1: 100%|██████████| 50/50 [00:04<00:00, 11.55it/s]        | 1/42 [00:04<03:24,  4.99s/it]\n",
      "Batch 2: 100%|██████████| 50/50 [00:04<00:00, 12.21it/s]        | 2/42 [00:09<03:05,  4.64s/it]\n",
      "Batch 3: 100%|██████████| 50/50 [00:04<00:00, 11.00it/s]        | 3/42 [00:13<02:52,  4.42s/it]\n",
      "Batch 4: 100%|██████████| 50/50 [00:05<00:00,  9.89it/s]        | 4/42 [00:18<02:50,  4.49s/it]\n",
      "Batch 5:  48%|████▊     | 24/50 [00:02<00:02, 10.83it/s]        | 5/42 [00:23<02:55,  4.73s/it]"
     ]
    }
   ],
   "source": [
    "# model_path = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "# prompt_func_list = [prompt_llama3]\n",
    "# run = ModelResponder(model_path, exam_list_local, prompt_func_list, inst_list, llama3=True)\n",
    "# run.process_files()\n",
    "# run.delete()\n",
    "\n",
    "# model_path = \"CohereForAI/c4ai-command-r-plus\"\n",
    "# prompt_func_list = [prompt_command]\n",
    "# run = ModelResponder(model_path, exam_list_local, prompt_func_list, inst_list)\n",
    "# run.process_files()\n",
    "# # run.delete()\n",
    "\n",
    "# model_path = \"Qwen/Qwen2-72B-Instruct\"\n",
    "# prompt_func_list = [prompt_qwen2]\n",
    "# run = ModelResponder(model_path, exam_list_local, prompt_func_list, inst_list, qwen=True)\n",
    "# run.process_files()\n",
    "# run.delete()\n",
    "\n",
    "# model_path = \"CohereForAI/c4ai-command-r-v01\"\n",
    "# prompt_func_list = [prompt_command]\n",
    "# run = ModelResponder(model_path, exam_list_local, prompt_func_list, inst_list)\n",
    "# run.process_files()\n",
    "# run.delete()\n",
    "\n",
    "# model_path = \"01-ai/Yi-1.5-34B-Chat\"\n",
    "# prompt_func_list = [prompt_yi]\n",
    "# run = ModelResponder(model_path, exam_list_local, prompt_func_list, inst_list, yi=True)\n",
    "# run.process_files()\n",
    "# run.delete()\n",
    "\n",
    "# model_path = \"moreh/MoMo-72B-lora-1.8.7-DPO\"\n",
    "# prompt_func_list = [prompt_momo]\n",
    "# run = ModelResponder(model_path, exam_list_local, prompt_func_list, inst_list)\n",
    "# run.process_files()\n",
    "# run.delete()\n",
    "\n",
    "# model_path = \"upstage/SOLAR-10.7B-Instruct-v1.0\"\n",
    "# prompt_func_list = [prompt_solar]\n",
    "# run = ModelResponder(model_path, exam_list_local, prompt_func_list, inst_list)\n",
    "# run.process_files()\n",
    "# run.delete()\n",
    "\n",
    "# model_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# prompt_func_list = [prompt_llama3]\n",
    "# run = ModelResponder(model_path, exam_list_local, prompt_func_list, inst_list, llama3=True)\n",
    "# run.process_files()\n",
    "# run.delete()\n",
    "\n",
    "# model_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# prompt_func_list = [prompt_llama2]\n",
    "# run = ModelResponder(model_path, exam_list_local, prompt_func_list, inst_list)\n",
    "# run.process_files()\n",
    "# run.delete()\n",
    "\n",
    "# model_path = \"microsoft/Phi-3-medium-4k-instruct\"\n",
    "# prompt_func_list = [prompt_phi3]\n",
    "# run = ModelResponder(model_path, exam_list_local, prompt_func_list, inst_list, qwen=True)\n",
    "# run.process_files()\n",
    "# run.delete()\n",
    "\n",
    "# model_path = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "# prompt_func_list = [prompt_llama2]\n",
    "# run = ModelResponder(model_path, exam_list_local, prompt_func_list, inst_list)\n",
    "# run.process_files()\n",
    "# run.delete()\n",
    "\n",
    "# model_path = \"upstage/SOLAR-0-70b-16bit\"\n",
    "# prompt_func_list = [prompt_solar]\n",
    "# run = ModelResponder(model_path, exam_list_local, prompt_func_list, inst_list)\n",
    "# run.process_files()\n",
    "# run.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
