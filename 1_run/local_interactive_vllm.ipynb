{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 240626\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from itertools import product\n",
    "import gc\n",
    "from format import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelResponder:\n",
    "    def __init__(self, model_path, ques_path_list, prompt_func_list, inst_list, path=None, quant=False, llama3=False, qwen=False, yi=False):\n",
    "        self.sampling_params = SamplingParams(temperature=0)\n",
    "        self.batch_size = 200\n",
    "        self.device_num = torch.cuda.device_count()\n",
    "        self.model = LLM(model=model_path, tensor_parallel_size=self.device_num, disable_custom_all_reduce=True)\n",
    "        if path is not None:\n",
    "            self.path = path\n",
    "        else:\n",
    "            self.path = os.path.basename(model_path)\n",
    "\n",
    "        indexed_ques_paths = list(enumerate(ques_path_list))\n",
    "        indexed_prompt_funcs = list(enumerate(prompt_func_list))\n",
    "        indexed_insts = list(enumerate(inst_list))\n",
    "\n",
    "        self.combinations = list(product(indexed_ques_paths, indexed_prompt_funcs, indexed_insts))\n",
    "\n",
    "    def process_files(self):\n",
    "        for (ques_idx, ques_path), (prompt_idx, prompt_func), (inst_idx, inst) in self.combinations:\n",
    "            with open(ques_path) as f:\n",
    "                exam = json.load(f)\n",
    "            filename=f'output/{self.path} [f{prompt_idx+1}_p{context_idx+1}_q{ques_idx+1}].json'\n",
    "            dataset = ExamDataset(exam, inst, prompt_func)\n",
    "            dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=0)\n",
    "            with tqdm(total=len(dataloader),desc=filename, leave=True, position=0) as pbar:\n",
    "                for i, batch in enumerate(dataloader):\n",
    "                    results=[]\n",
    "                    attempt = 0  # Track the number of attempts for this question\n",
    "                    while attempt < 2:  # Allow up to 2 attempts\n",
    "                        try:\n",
    "                            outputs = self.model.generate(batch['input'],sampling_params=self.sampling_params)\n",
    "                            output_text = []\n",
    "                            for output in outputs:\n",
    "                                generated_text = output.outputs[0].text\n",
    "                                output_text.append(generated_text)\n",
    "                            batch['response'] = output_text\n",
    "                            ques_dict = [{key: batch[key][i].tolist() if isinstance(batch[key][i], torch.Tensor) else batch[key][i] for key in batch} for i in range(len(batch['input']))]\n",
    "\n",
    "                            results.append(ques_dict)\n",
    "                            break  # Break the loop if successful\n",
    "                        except RuntimeError as e:\n",
    "                            if \"CUDA out of memory\" in str(e):\n",
    "                                print(f\"Attempt {attempt + 1}: CUDA OOM on {filename}. Trying again after clearing cache.\")\n",
    "                                torch.cuda.empty_cache()  # Try to free some memory\n",
    "                                attempt += 1\n",
    "                                if attempt == 2:\n",
    "                                    print(f\"Skipping {filename} after repeated OOM errors.\")\n",
    "                            else:\n",
    "                                raise  # Re-raise exception if it's not a CUDA OOM error\n",
    "\n",
    "                    if os.path.exists(filename):\n",
    "                        with open(filename, 'r') as file:\n",
    "                            resp = json.load(file)\n",
    "                    else:\n",
    "                        resp = []\n",
    "                        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "                    resp += results\n",
    "                    with open(filename, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(resp, f, indent=4, ensure_ascii=False)\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "    def delete(self):\n",
    "        del self.model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst = \"\\\n",
    "You are taking the Pharmacist Licensing Examination. \\\n",
    "Read the question carefully and use your judgment to select one of the options beginning with '①, ②, ③, ④, ⑤' as an answer. \\\n",
    "Submit only one symbol from '①, ②, ③, ④, ⑤' without any explanation, as this is an OMR exam.\\\n",
    "\"\n",
    "\n",
    "inst_list = [inst]\n",
    "\n",
    "ques_path_list = [\n",
    "    r'/home/hwjang/project/LLM/240625/exams_final.json',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     model_path = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "#     prompt_func_list = [prompt_llama3]\n",
    "#     run = ModelResponder(model_path, ques_path_list, prompt_func_list, inst_list)\n",
    "#     run.process_files()\n",
    "#     run.delete()\n",
    "\n",
    "# def main():\n",
    "#     model_path = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "#     prompt_func_list = [prompt_llama2]\n",
    "#     run = ModelResponder(model_path, ques_path_list, prompt_func_list, inst_list)\n",
    "#     run.process_files()\n",
    "#     run.delete()\n",
    "\n",
    "# def main():\n",
    "#     model_path = \"/home/hwjang/project/LLM/data/SOLAR-0-70b-16bit\"\n",
    "#     prompt_func_list = [prompt_solar]\n",
    "#     run = ModelResponder(model_path, ques_path_list, prompt_func_list, inst_list)\n",
    "#     run.process_files()\n",
    "#     run.delete()\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     model_path = \"Qwen/Qwen2-72B-Instruct\"\n",
    "#     prompt_func_list = [prompt_qwen2]\n",
    "#     run = ModelResponder(model_path, ques_path_list, prompt_func_list, inst_list, qwen=True)\n",
    "#     run.process_files()\n",
    "#     run.delete()\n",
    "\n",
    "# def main():\n",
    "#     model_path = \"CohereForAI/c4ai-command-r-plus\"\n",
    "#     prompt_func_list = [prompt_command]\n",
    "#     run = ModelResponder(model_path, ques_path_list, prompt_func_list, inst_list)\n",
    "#     run.process_files()\n",
    "#     run.delete()\n",
    "\n",
    "# def main():\n",
    "#     model_path = \"CohereForAI/c4ai-command-r-v01\"\n",
    "#     prompt_func_list = [prompt_command]\n",
    "#     run = ModelResponder(model_path, ques_path_list, prompt_func_list, inst_list)\n",
    "#     run.process_files()\n",
    "#     run.delete()\n",
    "\n",
    "# def main():\n",
    "#     model_path = \"01-ai/Yi-1.5-34B-Chat\"\n",
    "#     prompt_func_list = [prompt_yi]\n",
    "#     run = ModelResponder(model_path, ques_path_list, prompt_func_list, inst_list, yi=True)\n",
    "#     run.process_files()\n",
    "#     run.delete()\n",
    "\n",
    "# def main():\n",
    "#     model_path = \"moreh/MoMo-72B-lora-1.8.7-DPO\"\n",
    "#     prompt_func_list = [prompt_momo]\n",
    "#     run = ModelResponder(model_path, ques_path_list, prompt_func_list, inst_list)\n",
    "#     run.process_files()\n",
    "#     run.delete()\n",
    "\n",
    "# def main():\n",
    "#     model_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "#     prompt_func_list = [prompt_llama2]\n",
    "#     run = ModelResponder(model_path, ques_path_list, prompt_func_list, inst_list)\n",
    "#     run.process_files()\n",
    "#     run.delete()\n",
    "\n",
    "# def main():\n",
    "#     model_path = \"upstage/SOLAR-10.7B-Instruct-v1.0\"\n",
    "#     prompt_func_list = [prompt_solar]\n",
    "#     run = ModelResponder(model_path, ques_path_list, prompt_func_list, inst_list)\n",
    "#     run.process_files()\n",
    "#     run.delete()\n",
    "\n",
    "def main():\n",
    "    model_path = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    prompt_func_list = [prompt_llama3]\n",
    "    run = ModelResponder(model_path, ques_path_list, prompt_func_list, inst_list)\n",
    "    run.process_files()\n",
    "    run.delete()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
