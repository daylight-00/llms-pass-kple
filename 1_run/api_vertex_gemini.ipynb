{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 240630\n",
    "\n",
    "from glob import glob\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from itertools import product\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
    "import vertexai.preview.generative_models as generative_models\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from format import *\n",
    "\n",
    "vertexai.init(project='phonic-impact-421908', location=\"asia-northeast3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelResponder:\n",
    "    def __init__(self, model_path, ques_path_list, context_list, prompt_func_list=None, path=None):\n",
    "        self.batch_size = 350\n",
    "        self.model_path = model_path\n",
    "        if path is not None:\n",
    "            self.path = path\n",
    "        else:\n",
    "            self.path = os.path.basename(model_path)\n",
    "\n",
    "        if not prompt_func_list:\n",
    "            default_prompt_func = lambda inst,ques: f\"{ques}\"\n",
    "            prompt_func_list = [default_prompt_func]\n",
    "        self.prompt_func_list = prompt_func_list\n",
    "\n",
    "        indexed_ques_paths = list(enumerate(ques_path_list))\n",
    "        indexed_prompt_funcs = list(enumerate(self.prompt_func_list))\n",
    "        indexed_contexts = list(enumerate(context_list))\n",
    "\n",
    "        self.combinations = list(product(indexed_ques_paths, indexed_prompt_funcs, indexed_contexts))\n",
    "        \n",
    "        self.generation_config = GenerationConfig(\n",
    "            temperature=0,\n",
    "            candidate_count=1,\n",
    "        )\n",
    "        self.safety_settings = {\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        }\n",
    "    def process_and_update(self, ques_dict, context, prompt_func, pbar):\n",
    "        max_retries = 5\n",
    "        retries = 0\n",
    "        ques=ques_dict['input']\n",
    "        self.model = GenerativeModel(\n",
    "            self.model_path,\n",
    "            system_instruction=[context]\n",
    "            )\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                responses = self.model.generate_content(\n",
    "                    contents=ques,\n",
    "                    generation_config=self.generation_config,\n",
    "                    safety_settings=self.safety_settings,\n",
    "                )\n",
    "                pbar.update(1)\n",
    "                output = responses.text\n",
    "                ques_dict['response'] = output\n",
    "                return ques_dict\n",
    "            except Exception as e:\n",
    "                current = f\"{ques_dict['year']}-{ques_dict['session']}-{ques_dict['question_number']}\"\n",
    "                if \"Quota\" or \"quota\" in str(e):\n",
    "                    time.sleep(30)\n",
    "                elif \"candidate is likely blocked\" in str(e):\n",
    "                    retries += 1\n",
    "                    print(f\"Candidate blocked '{current}'\")\n",
    "                else:\n",
    "                    retries += 1\n",
    "                    print(e)\n",
    "                    print(f\"Retrying '{current}'... {retries}/{max_retries}\")\n",
    "                    time.sleep(10)\n",
    "                if retries == max_retries:\n",
    "                    print(f\"Failed to generate response for '{current}', skipping...\")\n",
    "                    return None\n",
    "                \n",
    "    def process_files(self):\n",
    "        for (ques_idx, ques_path), (prompt_idx, prompt_func), (context_idx, inst) in self.combinations:\n",
    "            with open(ques_path) as f:\n",
    "                exam = json.load(f)\n",
    "            filename=f'output/{self.path} [f{prompt_idx+1}_p{context_idx+1}_q{ques_idx+1}].json'\n",
    "            dataset = ExamDataset(exam, inst, prompt_func)\n",
    "            dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "            with tqdm(total=len(dataloader),desc=filename, leave=True, position=1) as pbar:\n",
    "                for i, batch in enumerate(dataloader):\n",
    "                    with ThreadPoolExecutor() as executor:\n",
    "                        with tqdm(total=len(batch), desc=f\"Batch {i}\", leave=True, position=0) as batch_pbar:\n",
    "                            results = list(executor.map(lambda ques: self.process_and_update(ques, inst, prompt_func, batch_pbar), batch))\n",
    "                    if os.path.exists(filename):\n",
    "                        with open(filename, 'r') as file:\n",
    "                            resp = json.load(file)\n",
    "                    else:\n",
    "                        resp = []\n",
    "                        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "                    for result in results:\n",
    "                        resp.append(result)\n",
    "                    with open(filename, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(resp, f, indent=4, ensure_ascii=False)\n",
    "                    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 0: 100%|██████████| 350/350 [05:10<00:00,  1.13it/s]| 0/6 [00:00<?, ?it/s]\n",
      "Batch 1: 100%|██████████| 350/350 [04:51<00:00,  1.20it/s]| 1/6 [05:10<25:53, 310.63s/it]\n",
      "Batch 2: 100%|██████████| 350/350 [04:52<00:00,  1.20it/s]| 2/6 [10:01<19:56, 299.22s/it]\n",
      "Batch 3: 100%|██████████| 350/350 [03:41<00:00,  1.58it/s]| 3/6 [14:54<14:48, 296.18s/it]\n",
      "Batch 4: 100%|██████████| 350/350 [05:10<00:00,  1.13it/s]| 4/6 [18:36<08:53, 266.87s/it]\n",
      "Batch 5: 100%|██████████| 350/350 [04:53<00:00,  1.19it/s]| 5/6 [23:47<04:42, 282.68s/it]\n",
      "output/gemini-1.5-pro-001 [f1_p1_q1].json: 100%|██████████| 6/6 [28:40<00:00, 286.79s/it]\n",
      "Batch 0: 100%|██████████| 350/350 [04:34<00:00,  1.27it/s]| 0/6 [00:00<?, ?it/s]\n",
      "Batch 1: 100%|██████████| 350/350 [04:50<00:00,  1.20it/s]| 1/6 [04:34<22:52, 274.56s/it]\n",
      "Batch 2: 100%|██████████| 350/350 [05:07<00:00,  1.14it/s]| 2/6 [09:25<18:56, 284.09s/it]\n",
      "Batch 3: 100%|██████████| 350/350 [05:22<00:00,  1.08it/s]| 3/6 [14:32<14:43, 294.62s/it]\n",
      "Batch 4: 100%|██████████| 350/350 [04:46<00:00,  1.22it/s]| 4/6 [19:55<10:11, 305.80s/it]\n",
      "Batch 5: 100%|██████████| 350/350 [04:21<00:00,  1.34it/s]| 5/6 [24:41<04:58, 298.82s/it]\n",
      "output/gemini-1.5-pro-001 [f1_p1_q2].json: 100%|██████████| 6/6 [29:03<00:00, 290.60s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path = 'gemini-1.5-pro-001' # 2024-05-24\n",
    "run = ModelResponder(model_path, exam_list, inst_list)\n",
    "run.process_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = 'gemini-1.5-pro-preview-0514' # 2024-05-14\n",
    "# run = ModelResponder(model_path, ques_path_list, inst_list)\n",
    "# run.process_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 0: 360it [05:17,  1.13it/s]                         | 0/6 [00:00<?, ?it/s]\n",
      "Batch 1: 100%|██████████| 350/350 [03:55<00:00,  1.49it/s]| 1/6 [05:17<26:29, 317.91s/it]\n",
      "Batch 2: 351it [03:39,  1.60it/s]                         | 2/6 [09:13<17:57, 269.27s/it]\n",
      "Batch 3: 351it [03:06,  1.88it/s]                         | 3/6 [12:52<12:19, 246.52s/it]\n",
      "Batch 4: 100%|██████████| 350/350 [03:05<00:00,  1.88it/s]| 4/6 [15:59<07:25, 222.96s/it]\n",
      "Batch 5: 354it [05:36,  1.05it/s]                         | 5/6 [19:05<03:29, 209.57s/it]\n",
      "output/gemini-1.0-pro-002 [f1_p1_q1].json: 100%|██████████| 6/6 [24:41<00:00, 246.89s/it]\n",
      "Batch 0: 355it [06:38,  1.12s/it]                         | 0/6 [00:00<?, ?it/s]\n",
      "Batch 1: 357it [07:16,  1.22s/it]                         | 1/6 [06:38<33:12, 398.51s/it]\n",
      "Batch 2: 353it [07:01,  1.19s/it]                         | 2/6 [13:55<28:04, 421.00s/it]\n",
      "Batch 3: 487it [1:17:30, 31.89s/it]                       | 3/6 [20:56<21:03, 421.28s/it]"
     ]
    }
   ],
   "source": [
    "model_path = 'gemini-1.0-pro-002' # 2024-04-09\n",
    "run = ModelResponder(model_path, exam_list, inst_list)\n",
    "run.process_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
