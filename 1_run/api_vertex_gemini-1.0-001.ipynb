{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 240630\n",
    "\n",
    "from glob import glob\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from itertools import product\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
    "import vertexai.preview.generative_models as generative_models\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from format import *\n",
    "\n",
    "vertexai.init(project='phonic-impact-421908', location=\"asia-northeast3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelResponder:\n",
    "    def __init__(self, model_path, ques_path_list, context_list, prompt_func_list=None, path=None):\n",
    "        self.batch_size = 350\n",
    "        self.model_path = model_path\n",
    "        if path is not None:\n",
    "            self.path = path\n",
    "        else:\n",
    "            self.path = os.path.basename(model_path)\n",
    "\n",
    "        if not prompt_func_list:\n",
    "            default_prompt_func = lambda inst,ques: f\"{ques}\"\n",
    "            prompt_func_list = [default_prompt_func]\n",
    "        self.prompt_func_list = prompt_func_list\n",
    "\n",
    "        indexed_ques_paths = list(enumerate(ques_path_list))\n",
    "        indexed_prompt_funcs = list(enumerate(self.prompt_func_list))\n",
    "        indexed_contexts = list(enumerate(context_list))\n",
    "\n",
    "        self.combinations = list(product(indexed_ques_paths, indexed_prompt_funcs, indexed_contexts))\n",
    "        \n",
    "        self.generation_config = GenerationConfig(\n",
    "            temperature=0,\n",
    "            candidate_count=1,\n",
    "        )\n",
    "        self.safety_settings = {\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        }\n",
    "    def process_and_update(self, ques_dict, context, prompt_func, pbar):\n",
    "        max_retries = 5\n",
    "        retries = 0\n",
    "        ques=ques_dict['input']\n",
    "        self.model = GenerativeModel(\n",
    "            self.model_path,\n",
    "            # system_instruction=[context]\n",
    "            )\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                responses = self.model.generate_content(\n",
    "                    contents=[context,ques],\n",
    "                    generation_config=self.generation_config,\n",
    "                    safety_settings=self.safety_settings,\n",
    "                )\n",
    "                pbar.update(1)\n",
    "                output = responses.text\n",
    "                ques_dict['response'] = output\n",
    "                return ques_dict\n",
    "            except Exception as e:\n",
    "                current = f\"{ques_dict['year']}-{ques_dict['session']}-{ques_dict['question_number']}\"\n",
    "                if \"Quota\" or \"quota\" in str(e):\n",
    "                    time.sleep(30)\n",
    "                elif \"candidate is likely blocked\" in str(e):\n",
    "                    retries += 1\n",
    "                    print(f\"Candidate blocked '{current}'\")\n",
    "                else:\n",
    "                    retries += 1\n",
    "                    print(e)\n",
    "                    print(f\"Retrying '{current}'... {retries}/{max_retries}\")\n",
    "                    time.sleep(10)\n",
    "                if retries == max_retries:\n",
    "                    print(f\"Failed to generate response for '{current}', skipping...\")\n",
    "                    return None\n",
    "                \n",
    "    def process_files(self):\n",
    "        for (ques_idx, ques_path), (prompt_idx, prompt_func), (context_idx, inst) in self.combinations:\n",
    "            with open(ques_path) as f:\n",
    "                exam = json.load(f)\n",
    "            filename=f'output/{self.path} [f{prompt_idx+1}_p{context_idx+1}_q{ques_idx+1}].json'\n",
    "            dataset = ExamDataset(exam, inst, prompt_func)\n",
    "            dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "            with tqdm(total=len(dataloader),desc=filename, leave=True, position=0) as pbar:\n",
    "                for i, batch in enumerate(dataloader):\n",
    "                    with ThreadPoolExecutor() as executor:\n",
    "                        with tqdm(total=len(batch), desc=f\"Batch {i}\", leave=True, position=0) as batch_pbar:\n",
    "                            results = list(executor.map(lambda ques: self.process_and_update(ques, inst, prompt_func, batch_pbar), batch))\n",
    "                    if os.path.exists(filename):\n",
    "                        with open(filename, 'r') as file:\n",
    "                            resp = json.load(file)\n",
    "                    else:\n",
    "                        resp = []\n",
    "                        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "                    for result in results:\n",
    "                        resp.append(result)\n",
    "                    with open(filename, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(resp, f, indent=4, ensure_ascii=False)\n",
    "                    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 0:  17%|█▋        | 61/350 [00:07<00:11, 24.56it/s] | 0/6 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "model_path = 'gemini-1.0-pro-001' # 2024-02-15\n",
    "run = ModelResponder(model_path, exam_list, inst_list)\n",
    "run.process_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
