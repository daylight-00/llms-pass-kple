{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 240630\n",
    "\n",
    "from glob import glob\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from itertools import product\n",
    "import vertexai\n",
    "from vertexai.language_models import ChatModel, InputOutputTextPair\n",
    "import vertexai.preview.generative_models as generative_models\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from format import *\n",
    "\n",
    "vertexai.init(project='phonic-impact-421908', location=\"asia-northeast3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelResponder:\n",
    "    def __init__(self, model_path, ques_path_list, context_list, prompt_func_list=None, path=None):\n",
    "        self.batch_size = 350\n",
    "        self.model_path = model_path\n",
    "        if path is not None:\n",
    "            self.path = path\n",
    "        else:\n",
    "            self.path = os.path.basename(model_path)\n",
    "\n",
    "        if not prompt_func_list:\n",
    "            default_prompt_func = lambda inst,ques: f\"{ques}\"\n",
    "            prompt_func_list = [default_prompt_func]\n",
    "        self.prompt_func_list = prompt_func_list\n",
    "\n",
    "        indexed_ques_paths = list(enumerate(ques_path_list))\n",
    "        indexed_prompt_funcs = list(enumerate(self.prompt_func_list))\n",
    "        indexed_contexts = list(enumerate(context_list))\n",
    "\n",
    "        self.combinations = list(product(indexed_ques_paths, indexed_prompt_funcs, indexed_contexts))\n",
    "        \n",
    "        self.safety_settings = {\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "            generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        }\n",
    "    def process_and_update(self, ques_dict, context, prompt_func, pbar):\n",
    "        max_retries = 5\n",
    "        retries = 0\n",
    "        ques=ques_dict['input']\n",
    "        self.chat_model = ChatModel.from_pretrained(self.model_path)\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                chat = self.chat_model.start_chat(context=context)\n",
    "                responses = chat.send_message(ques, temperature=0)\n",
    "                pbar.update(1)\n",
    "                output = responses.text\n",
    "                ques_dict['response'] = output\n",
    "                return ques_dict\n",
    "            except Exception as e:\n",
    "                current = f\"{ques_dict['year']}-{ques_dict['session']}-{ques_dict['question_number']}\"\n",
    "                if \"Quota\" or \"quota\" in str(e):\n",
    "                    time.sleep(30)\n",
    "                elif \"candidate is likely blocked\" in str(e):\n",
    "                    retries += 1\n",
    "                    print(f\"Candidate blocked '{current}'\")\n",
    "                else:\n",
    "                    retries += 1\n",
    "                    print(e)\n",
    "                    print(f\"Retrying '{current}'... {retries}/{max_retries}\")\n",
    "                    time.sleep(10)\n",
    "                if retries == max_retries:\n",
    "                    print(f\"Failed to generate response for '{current}', skipping...\")\n",
    "                    ques_dict['response'] = None\n",
    "                    return ques_dict\n",
    "            except RetryError as e:\n",
    "                time.sleep(60)\n",
    "    def process_files(self):\n",
    "        for (ques_idx, ques_path), (prompt_idx, prompt_func), (context_idx, inst) in self.combinations:\n",
    "            with open(ques_path) as f:\n",
    "                exam = json.load(f)\n",
    "            filename=f'output/{self.path} [f{prompt_idx+1}_p{context_idx+1}_q{ques_idx+1}].json'\n",
    "            dataset = ExamDataset(exam, inst, prompt_func)\n",
    "            dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "            with tqdm(total=len(dataloader),desc=filename, leave=True, position=1) as pbar:\n",
    "                for i, batch in enumerate(dataloader):\n",
    "                    with ThreadPoolExecutor() as executor:\n",
    "                        with tqdm(total=len(batch), desc=f\"Batch {i}\", leave=True, position=0) as batch_pbar:\n",
    "                            results = list(executor.map(lambda ques: self.process_and_update(ques, inst, prompt_func, batch_pbar), batch))\n",
    "                    if os.path.exists(filename):\n",
    "                        with open(filename, 'r') as file:\n",
    "                            resp = json.load(file)\n",
    "                    else:\n",
    "                        resp = []\n",
    "                        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "                    for result in results:\n",
    "                        resp.append(result)\n",
    "                    with open(filename, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(resp, f, indent=4, ensure_ascii=False)\n",
    "                    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 0:   0%|          | 0/350 [00:00<?, ?it/s]      | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 0: 100%|██████████| 350/350 [00:32<00:00, 10.92it/s]\n",
      "Batch 1: 100%|██████████| 350/350 [00:48<00:00,  7.17it/s]6 [00:32<02:40, 32.06s/it]\n",
      "Batch 2: 100%|██████████| 350/350 [00:26<00:00, 13.13it/s]6 [01:20<02:47, 41.92s/it]\n",
      "Batch 3: 100%|██████████| 350/350 [00:49<00:00,  7.00it/s]6 [01:47<01:44, 34.96s/it]\n",
      "Batch 4: 100%|██████████| 350/350 [00:25<00:00, 13.88it/s]6 [02:37<01:21, 40.90s/it]\n",
      "Batch 5: 100%|██████████| 350/350 [00:24<00:00, 14.33it/s]6 [03:02<00:35, 35.24s/it]\n",
      "output/chat-bison@001 [f1_p1_q1].json: 100%|██████████| 6/6 [03:27<00:00, 34.53s/it]\n",
      "Batch 0: 100%|██████████| 350/350 [01:09<00:00,  5.03it/s]6 [00:00<?, ?it/s]\n",
      "Batch 1: 100%|██████████| 350/350 [00:23<00:00, 14.66it/s]6 [01:09<05:47, 69.55s/it]\n",
      "Batch 2: 100%|██████████| 350/350 [00:25<00:00, 13.79it/s]6 [01:33<02:50, 42.68s/it]\n",
      "Batch 3: 100%|██████████| 350/350 [00:57<00:00,  6.14it/s]6 [01:58<01:44, 34.78s/it]\n",
      "Batch 4: 100%|██████████| 350/350 [00:23<00:00, 14.76it/s]6 [02:55<01:27, 43.57s/it]\n",
      "Batch 5: 100%|██████████| 350/350 [00:28<00:00, 12.21it/s]6 [03:19<00:36, 36.41s/it]\n",
      "output/chat-bison@001 [f1_p1_q2].json: 100%|██████████| 6/6 [03:48<00:00, 38.04s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path = 'chat-bison@001' # 2023-07-10\n",
    "\n",
    "run = ModelResponder(model_path, exam_list, inst_list)\n",
    "run.process_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 0:   0%|          | 0/350 [00:00<?, ?it/s]      | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 0: 100%|██████████| 350/350 [00:25<00:00, 13.56it/s]\n",
      "Batch 1: 100%|██████████| 350/350 [01:37<00:00,  3.60it/s]6 [00:25<02:09, 25.82s/it]\n",
      "Batch 2: 100%|██████████| 350/350 [00:21<00:00, 15.98it/s]6 [02:03<04:31, 67.89s/it]\n",
      "Batch 3: 100%|██████████| 350/350 [01:56<00:00,  3.00it/s]6 [02:25<02:20, 46.90s/it]\n",
      "Batch 4: 100%|██████████| 350/350 [00:20<00:00, 17.42it/s]6 [04:21<02:28, 74.39s/it]\n",
      "Batch 5: 100%|██████████| 350/350 [00:54<00:00,  6.36it/s]6 [04:41<00:54, 54.82s/it]\n",
      "output/chat-bison@002 [f1_p1_q1].json: 100%|██████████| 6/6 [05:36<00:00, 56.13s/it]\n",
      "Batch 0: 100%|██████████| 350/350 [00:19<00:00, 17.70it/s]6 [00:00<?, ?it/s]\n",
      "Batch 1: 100%|██████████| 350/350 [00:21<00:00, 16.05it/s]6 [00:19<01:38, 19.78s/it]\n",
      "Batch 2: 100%|██████████| 350/350 [00:20<00:00, 17.03it/s]6 [00:41<01:23, 20.98s/it]\n",
      "Batch 3: 100%|██████████| 350/350 [01:18<00:00,  4.45it/s]6 [01:02<01:02, 20.80s/it]\n",
      "Batch 4: 100%|██████████| 350/350 [00:20<00:00, 17.00it/s]6 [02:20<01:27, 43.66s/it]\n",
      "Batch 5: 100%|██████████| 350/350 [00:20<00:00, 17.21it/s]6 [02:41<00:35, 35.35s/it]\n",
      "output/chat-bison@002 [f1_p1_q2].json: 100%|██████████| 6/6 [03:01<00:00, 30.32s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path = 'chat-bison@002' # 2023-12-06\n",
    "\n",
    "run = ModelResponder(model_path, exam_list, inst_list)\n",
    "run.process_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
