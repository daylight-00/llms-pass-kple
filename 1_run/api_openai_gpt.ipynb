{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 240630\n",
    "from glob import glob\n",
    "import pickle\n",
    "from openai  import OpenAI\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from itertools import product\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from format import *\n",
    "\n",
    "client = OpenAI(api_key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelResponder:\n",
    "    def __init__(self, model_path, ques_path_list, context_list, prompt_func_list=None, path=None):\n",
    "        self.batch_size = 350\n",
    "        self.model_path = model_path\n",
    "        if path is not None:\n",
    "            self.path = path\n",
    "        else:\n",
    "            self.path = os.path.basename(model_path)\n",
    "\n",
    "        if not prompt_func_list:\n",
    "            default_prompt_func = lambda inst,ques: f\"{ques}\"\n",
    "            prompt_func_list = [default_prompt_func]\n",
    "        self.prompt_func_list = prompt_func_list\n",
    "\n",
    "        indexed_ques_paths = list(enumerate(ques_path_list))\n",
    "        indexed_prompt_funcs = list(enumerate(self.prompt_func_list))\n",
    "        indexed_contexts = list(enumerate(context_list))\n",
    "        self.combinations = list(product(indexed_ques_paths, indexed_prompt_funcs, indexed_contexts))\n",
    "\n",
    "    def process_and_update(self, ques_dict, inst, prompt_func, batch_pbar):\n",
    "        max_retries = 5\n",
    "        retries = 0\n",
    "        ques=ques_dict['input']\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=self.model_path,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": inst},\n",
    "                        {\"role\": \"user\", \"content\": ques}\n",
    "                    ],\n",
    "                    temperature=0.0000000000000000000000000000000000000000001,\n",
    "                    top_p=0.0000000000000000000000000000000000000000001,\n",
    "                    seed=100,\n",
    "                    )\n",
    "                batch_pbar.update(1)\n",
    "                output = response.choices[0].message.content\n",
    "                ques_dict['response'] = output\n",
    "                return ques_dict\n",
    "            except Exception as e:\n",
    "                current = f\"{ques_dict['year']}-{ques_dict['session']}-{ques_dict['question_number']}\"\n",
    "                retries += 1\n",
    "                print(e)\n",
    "                print(f\"Retrying '{current}'... {retries}/{max_retries}\")\n",
    "                time.sleep(60)\n",
    "                if retries == max_retries:\n",
    "                    print(f\"Failed to generate response for '{current}', skipping...\")\n",
    "                    ques_dict['response'] = None\n",
    "                    return ques_dict\n",
    "                \n",
    "    def process_files(self):\n",
    "        for (ques_idx, ques_path), (prompt_idx, prompt_func), (context_idx, inst) in self.combinations:\n",
    "            with open(ques_path) as f:\n",
    "                exam = json.load(f)\n",
    "            filename=f'output/{self.path} [f{prompt_idx+1}_p{context_idx+1}_q{ques_idx+1}].json'\n",
    "            dataset = ExamDataset(exam, inst, prompt_func)\n",
    "            dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "            with tqdm(total=len(dataloader),desc=filename, leave=True, position=1) as pbar:\n",
    "                for i, batch in enumerate(dataloader):\n",
    "                    with ThreadPoolExecutor() as executor:\n",
    "                        with tqdm(total=len(batch), desc=f\"Batch {i}\", leave=True, position=0) as batch_pbar:\n",
    "                            results = list(executor.map(lambda ques: self.process_and_update(ques, inst, prompt_func, batch_pbar), batch))\n",
    "                    if os.path.exists(filename):\n",
    "                        with open(filename, 'r') as file:\n",
    "                            resp = json.load(file)\n",
    "                    else:\n",
    "                        resp = []\n",
    "                        os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "                    for result in results:\n",
    "                        resp.append(result)\n",
    "                    with open(filename, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(resp, f, indent=4, ensure_ascii=False)\n",
    "                    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 0: 100%|██████████| 350/350 [00:09<00:00, 38.27it/s] 0/6 [00:00<?, ?it/s]\n",
      "Batch 1: 100%|██████████| 350/350 [00:24<00:00, 14.06it/s] 1/6 [00:09<00:45,  9.16s/it]\n",
      "Batch 2: 100%|██████████| 350/350 [00:09<00:00, 36.10it/s] 2/6 [00:34<01:13, 18.42s/it]\n",
      "Batch 3: 100%|██████████| 350/350 [00:12<00:00, 27.17it/s] 3/6 [00:43<00:43, 14.44s/it]\n",
      "Batch 4: 100%|██████████| 350/350 [00:10<00:00, 32.47it/s] 4/6 [00:56<00:27, 13.84s/it]\n",
      "Batch 5: 100%|██████████| 350/350 [00:16<00:00, 21.13it/s] 5/6 [01:07<00:12, 12.75s/it]\n",
      "output/gpt-4o-2024-05-13 [f1_p1_q1].json: 100%|██████████| 6/6 [01:24<00:00, 14.02s/it]\n",
      "Batch 0: 100%|██████████| 350/350 [00:20<00:00, 17.49it/s] 0/6 [00:00<?, ?it/s]\n",
      "Batch 1: 100%|██████████| 350/350 [00:09<00:00, 37.36it/s] 1/6 [00:20<01:40, 20.02s/it]\n",
      "Batch 2: 100%|██████████| 350/350 [00:08<00:00, 38.90it/s] 2/6 [00:29<00:55, 13.76s/it]\n",
      "Batch 3: 100%|██████████| 350/350 [00:08<00:00, 39.85it/s] 3/6 [00:38<00:34, 11.60s/it]\n",
      "Batch 4: 100%|██████████| 350/350 [00:09<00:00, 36.38it/s] 4/6 [00:47<00:20, 10.50s/it]\n",
      "Batch 5: 100%|██████████| 350/350 [00:09<00:00, 37.84it/s] 5/6 [00:56<00:10, 10.19s/it]\n",
      "output/gpt-4o-2024-05-13 [f1_p1_q2].json: 100%|██████████| 6/6 [01:06<00:00, 11.04s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path=\"gpt-4o-2024-05-13\"\n",
    "\n",
    "run = ModelResponder(model_path, exam_list, inst_list)\n",
    "run.process_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 0: 100%|██████████| 350/350 [00:19<00:00, 17.50it/s]    | 0/6 [00:00<?, ?it/s]\n",
      "Batch 1: 100%|██████████| 350/350 [00:11<00:00, 29.74it/s]    | 1/6 [00:20<01:40, 20.01s/it]\n",
      "Batch 2: 100%|██████████| 350/350 [00:18<00:00, 18.65it/s]    | 2/6 [00:31<01:00, 15.17s/it]\n",
      "Batch 3: 100%|██████████| 350/350 [00:12<00:00, 26.99it/s]    | 3/6 [00:50<00:50, 16.83s/it]\n",
      "Batch 4: 100%|██████████| 350/350 [00:11<00:00, 29.59it/s]▋   | 4/6 [01:03<00:30, 15.32s/it]\n",
      "Batch 5: 100%|██████████| 350/350 [00:24<00:00, 14.44it/s]██▎ | 5/6 [01:15<00:14, 14.07s/it]\n",
      "output/gpt-4-turbo-2024-04-09 [f1_p1_q1].json: 100%|██████████| 6/6 [01:39<00:00, 16.63s/it]\n",
      "Batch 0: 100%|██████████| 350/350 [00:11<00:00, 29.21it/s]    | 0/6 [00:00<?, ?it/s]\n",
      "Batch 1: 100%|██████████| 350/350 [00:11<00:00, 29.31it/s]    | 1/6 [00:11<00:59, 11.99s/it]\n",
      "Batch 2: 100%|██████████| 350/350 [00:11<00:00, 29.42it/s]    | 2/6 [00:23<00:47, 11.97s/it]\n",
      "Batch 3: 100%|██████████| 350/350 [00:23<00:00, 14.72it/s]    | 3/6 [00:35<00:35, 11.95s/it]\n",
      "Batch 4: 100%|██████████| 350/350 [00:11<00:00, 29.75it/s]▋   | 4/6 [00:59<00:33, 16.63s/it]\n",
      "Batch 5: 100%|██████████| 350/350 [00:11<00:00, 29.39it/s]██▎ | 5/6 [01:11<00:14, 14.89s/it]\n",
      "output/gpt-4-turbo-2024-04-09 [f1_p1_q2].json: 100%|██████████| 6/6 [01:23<00:00, 13.91s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path=\"gpt-4-turbo-2024-04-09\"\n",
    "\n",
    "run = ModelResponder(model_path, exam_list, inst_list)\n",
    "run.process_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 0: 100%|██████████| 350/350 [00:11<00:00, 29.74it/s]| 0/6 [00:00<?, ?it/s]\n",
      "Batch 1: 100%|██████████| 350/350 [00:10<00:00, 33.64it/s]| 1/6 [00:11<00:58, 11.78s/it]\n",
      "Batch 2: 100%|██████████| 350/350 [00:10<00:00, 33.36it/s]| 2/6 [00:22<00:43, 10.98s/it]\n",
      "Batch 3: 100%|██████████| 350/350 [00:09<00:00, 35.14it/s]| 3/6 [00:32<00:32, 10.77s/it]\n",
      "Batch 4: 100%|██████████| 350/350 [00:09<00:00, 35.15it/s]| 4/6 [00:42<00:20, 10.46s/it]\n",
      "Batch 5: 100%|██████████| 350/350 [00:10<00:00, 34.91it/s]| 5/6 [00:52<00:10, 10.29s/it]\n",
      "output/gpt-3.5-turbo-0125 [f1_p1_q1].json: 100%|██████████| 6/6 [01:02<00:00, 10.46s/it]\n",
      "Batch 0: 100%|██████████| 350/350 [00:09<00:00, 36.76it/s]| 0/6 [00:00<?, ?it/s]\n",
      "Batch 1: 100%|██████████| 350/350 [00:09<00:00, 37.08it/s]| 1/6 [00:09<00:47,  9.53s/it]\n",
      "Batch 2: 100%|██████████| 350/350 [00:10<00:00, 34.96it/s]| 2/6 [00:18<00:37,  9.49s/it]\n",
      "Batch 3: 100%|██████████| 350/350 [00:09<00:00, 36.71it/s]| 3/6 [00:29<00:29,  9.74s/it]\n",
      "Batch 4: 100%|██████████| 350/350 [00:09<00:00, 35.56it/s]| 4/6 [00:38<00:19,  9.67s/it]\n",
      "Batch 5: 100%|██████████| 350/350 [00:09<00:00, 37.13it/s]| 5/6 [00:48<00:09,  9.74s/it]\n",
      "output/gpt-3.5-turbo-0125 [f1_p1_q2].json: 100%|██████████| 6/6 [00:57<00:00,  9.66s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path=\"gpt-3.5-turbo-0125\"\n",
    "\n",
    "run = ModelResponder(model_path, exam_list, inst_list)\n",
    "run.process_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 0: 100%|██████████| 350/350 [00:17<00:00, 19.66it/s]0:00<?, ?it/s]\n",
      "Batch 1: 100%|██████████| 350/350 [00:16<00:00, 21.03it/s]0:17<01:29, 17.81s/it]\n",
      "Batch 2: 100%|██████████| 350/350 [00:17<00:00, 19.70it/s]0:34<01:08, 17.14s/it]\n",
      "Batch 3: 100%|██████████| 350/350 [00:16<00:00, 20.79it/s]0:52<00:52, 17.43s/it]\n",
      "Batch 4: 100%|██████████| 350/350 [00:24<00:00, 14.53it/s]1:09<00:34, 17.21s/it]\n",
      "Batch 5: 100%|██████████| 350/350 [00:17<00:00, 20.37it/s]1:33<00:19, 19.70s/it]\n",
      "output/gpt-4-0613 [f1_p1_q1].json: 100%|██████████| 6/6 [01:50<00:00, 18.42s/it]\n",
      "Batch 0: 100%|██████████| 350/350 [00:15<00:00, 22.18it/s]0:00<?, ?it/s]\n",
      "Batch 1: 100%|██████████| 350/350 [00:14<00:00, 24.23it/s]0:15<01:18, 15.79s/it]\n",
      "Batch 2: 100%|██████████| 350/350 [00:14<00:00, 23.92it/s]0:30<01:00, 15.01s/it]\n",
      "Batch 3: 100%|██████████| 350/350 [00:15<00:00, 23.28it/s]0:44<00:44, 14.85s/it]\n",
      "Batch 4: 100%|██████████| 350/350 [00:15<00:00, 22.65it/s]0:59<00:29, 14.93s/it]\n",
      "Batch 5: 100%|██████████| 350/350 [00:14<00:00, 23.39it/s]1:15<00:15, 15.13s/it]\n",
      "output/gpt-4-0613 [f1_p1_q2].json: 100%|██████████| 6/6 [01:30<00:00, 15.08s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path=\"gpt-4-0613\"\n",
    "run = ModelResponder(model_path, exam_list, inst_list)\n",
    "run.process_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 0: 100%|██████████| 350/350 [00:13<00:00, 26.59it/s]| 0/6 [00:00<?, ?it/s]\n",
      "Batch 1: 100%|██████████| 350/350 [00:12<00:00, 28.77it/s]| 1/6 [00:13<01:05, 13.17s/it]\n",
      "Batch 2: 100%|██████████| 350/350 [00:12<00:00, 27.35it/s]| 2/6 [00:25<00:50, 12.59s/it]\n",
      "Batch 3: 100%|██████████| 350/350 [00:23<00:00, 14.64it/s]| 3/6 [00:38<00:38, 12.69s/it]\n",
      "Batch 4: 100%|██████████| 350/350 [00:12<00:00, 27.45it/s]| 4/6 [01:02<00:34, 17.13s/it]\n",
      "Batch 5: 100%|██████████| 350/350 [00:13<00:00, 26.16it/s]| 5/6 [01:14<00:15, 15.56s/it]\n",
      "output/gpt-3.5-turbo-0613 [f1_p1_q1].json: 100%|██████████| 6/6 [01:28<00:00, 14.72s/it]\n",
      "Batch 0: 100%|██████████| 350/350 [00:12<00:00, 27.23it/s]| 0/6 [00:00<?, ?it/s]\n",
      "Batch 1: 100%|██████████| 350/350 [00:13<00:00, 26.24it/s]| 1/6 [00:12<01:04, 12.87s/it]\n",
      "Batch 2: 100%|██████████| 350/350 [00:11<00:00, 30.94it/s]| 2/6 [00:26<00:52, 13.16s/it]\n",
      "Batch 3: 100%|██████████| 350/350 [00:13<00:00, 26.25it/s]| 3/6 [00:37<00:36, 12.33s/it]\n",
      "Batch 4: 100%|██████████| 350/350 [00:11<00:00, 30.42it/s]| 4/6 [00:50<00:25, 12.73s/it]\n",
      "Batch 5: 100%|██████████| 350/350 [00:12<00:00, 28.96it/s]| 5/6 [01:02<00:12, 12.30s/it]\n",
      "output/gpt-3.5-turbo-0613 [f1_p1_q2].json: 100%|██████████| 6/6 [01:14<00:00, 12.43s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path=\"gpt-3.5-turbo-0613\"\n",
    "run = ModelResponder(model_path, exam_list, inst_list)\n",
    "run.process_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path=\"gpt-4-0314\"\n",
    "# run = ModelResponder(model_path, ques_path_list, inst_list)\n",
    "# run.process_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 0: 100%|██████████| 350/350 [00:09<00:00, 36.84it/s]| 0/6 [00:00<?, ?it/s]\n",
      "Batch 1: 100%|██████████| 350/350 [00:09<00:00, 36.80it/s]| 1/6 [00:09<00:47,  9.51s/it]\n",
      "Batch 2: 100%|██████████| 350/350 [00:08<00:00, 39.28it/s]| 2/6 [00:19<00:38,  9.52s/it]\n",
      "Batch 3: 100%|██████████| 350/350 [00:08<00:00, 39.41it/s]| 3/6 [00:27<00:27,  9.25s/it]\n",
      "Batch 4: 100%|██████████| 350/350 [00:09<00:00, 35.04it/s]| 4/6 [00:36<00:18,  9.12s/it]\n",
      "Batch 5: 100%|██████████| 350/350 [00:09<00:00, 36.12it/s]| 5/6 [00:46<00:09,  9.44s/it]\n",
      "output/gpt-3.5-turbo-0301 [f1_p1_q1].json: 100%|██████████| 6/6 [00:56<00:00,  9.44s/it]\n",
      "Batch 0: 100%|██████████| 350/350 [00:08<00:00, 42.02it/s]| 0/6 [00:00<?, ?it/s]\n",
      "Batch 1: 100%|██████████| 350/350 [00:08<00:00, 42.31it/s]| 1/6 [00:08<00:41,  8.34s/it]\n",
      "Batch 2: 100%|██████████| 350/350 [00:08<00:00, 41.71it/s]| 2/6 [00:16<00:33,  8.31s/it]\n",
      "Batch 3: 100%|██████████| 350/350 [00:09<00:00, 37.86it/s]| 3/6 [00:25<00:25,  8.36s/it]\n",
      "Batch 4: 100%|██████████| 350/350 [00:09<00:00, 35.35it/s]| 4/6 [00:34<00:17,  8.72s/it]\n",
      "Batch 5: 100%|██████████| 350/350 [00:08<00:00, 41.14it/s]| 5/6 [00:44<00:09,  9.16s/it]\n",
      "output/gpt-3.5-turbo-0301 [f1_p1_q2].json: 100%|██████████| 6/6 [00:52<00:00,  8.80s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path=\"gpt-3.5-turbo-0301\"\n",
    "run = ModelResponder(model_path, exam_list, inst_list)\n",
    "run.process_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 0: 100%|██████████| 350/350 [00:17<00:00, 19.67it/s]\n",
      "Batch 1: 100%|██████████| 350/350 [00:16<00:00, 21.27it/s]\n",
      "Batch 2: 100%|██████████| 350/350 [00:16<00:00, 20.59it/s]\n",
      "Batch 3: 100%|██████████| 350/350 [00:17<00:00, 20.11it/s]\n",
      "Batch 4: 100%|██████████| 350/350 [00:16<00:00, 20.63it/s]\n",
      "Batch 5: 100%|██████████| 350/350 [00:26<00:00, 13.27it/s]\n",
      "output/gpt-4-1106-preview [f1_p1_q1].json: 100%|██████████| 6/6 [01:52<00:00, 18.69s/it]\n",
      "Batch 0: 100%|██████████| 350/350 [00:15<00:00, 22.11it/s]\n",
      "Batch 1: 100%|██████████| 350/350 [00:23<00:00, 14.79it/s]\n",
      "Batch 2: 100%|██████████| 350/350 [00:23<00:00, 14.91it/s]\n",
      "Batch 3: 100%|██████████| 350/350 [00:15<00:00, 22.74it/s]\n",
      "Batch 4: 100%|██████████| 350/350 [00:15<00:00, 22.15it/s]\n",
      "Batch 5: 100%|██████████| 350/350 [00:17<00:00, 19.86it/s]\n",
      "output/gpt-4-1106-preview [f1_p1_q2].json: 100%|██████████| 6/6 [01:51<00:00, 18.66s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path=\"gpt-4-1106-preview\"\n",
    "run = ModelResponder(model_path, exam_list, inst_list)\n",
    "run.process_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
